{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning: Summer 2020\n",
    "### Project 2\n",
    "\n",
    "# Sentiment Analysis of Amazon Product Reviews\n",
    "\n",
    "### Anshul Dabas and Anndi Russell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our original dataset consisted of approximately 34,000 product reviews (text) and ratings (out of 5 stars) for a variety of products on Amazon. It was downloaded from Kaggle; it's a subset of a larger original dataset available through Datafiniti. Datafiniti is a web scraping service with data available for download. Ethically, we are able to use this data since Amazon reviews are publically available without a login. \n",
    "\n",
    "We will predict, based on text of a review, whether the sentiment of the review of the product was positive or negative. For our data, we determined the sentiment based on the rating the reviewer gave; ratings of 1 or 2 were considered negative, and ratings of 4 or 5 were considered positive. We dropped reviews with a rating of 3, as they can be considered neutral and thus are not meaningful for our binary sentiment analysis. \n",
    "\n",
    "We chose this data because we were able to apply the sentiment analysis concept, since we could turn ratings into a positive or negative sentiment, resulting in labeled data for training a model.\n",
    "\n",
    "With this research, we could predict whether a customer felt positively or negatively about their product based on the text they use in a review. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data from csv (downloade dfrom archive here:https://www.kaggle.com/datafiniti/consumer-reviews-of-amazon-products/data#Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (1,10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "reviews_df = pd.read_csv('amazon_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select only relevant columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>This product so far has not disappointed. My c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>great for beginner or experienced person. Boug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Inexpensive tablet for him to use and learn on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>I've had my Fire HD 8 two weeks now and I love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I bought this for my grand daughter when she c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating                                             Review\n",
       "0     5.0  This product so far has not disappointed. My c...\n",
       "1     5.0  great for beginner or experienced person. Boug...\n",
       "2     5.0  Inexpensive tablet for him to use and learn on...\n",
       "3     4.0  I've had my Fire HD 8 two weeks now and I love...\n",
       "4     5.0  I bought this for my grand daughter when she c..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df = reviews_df[['reviews.rating','reviews.text']]\n",
    "reviews_df.columns=['Rating','Review']\n",
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a complete dataframe ready for processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating    33\n",
       "Review     1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rating    0\n",
       "Review    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.dropna(inplace=True)\n",
    "reviews_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A rating of 4 or 5 is considered positive and will be a '1'. A rating of 1 or 2 is considered negative and will be a '0'. Ratings of 3 will be dropped since they are neither positive nor negative.\n",
    "Mapping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingmap = {4:1,5:1,1:0,2:0}\n",
    "reviews_df['Sentiment'] = reviews_df['Rating'].map(ratingmap)\n",
    "reviews_df.drop(['Rating'], axis=1, inplace=True) #drop original rating column\n",
    "reviews_df.dropna(subset=['Sentiment'], inplace=True) #drop nulls-- these were 3s in the original column\n",
    "reviews_df['Sentiment']=reviews_df['Sentiment'].astype(int) #cast to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This product so far has not disappointed. My c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great for beginner or experienced person. Boug...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Inexpensive tablet for him to use and learn on...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I've had my Fire HD 8 two weeks now and I love...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I bought this for my grand daughter when she c...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0  This product so far has not disappointed. My c...          1\n",
       "1  great for beginner or experienced person. Boug...          1\n",
       "2  Inexpensive tablet for him to use and learn on...          1\n",
       "3  I've had my Fire HD 8 two weeks now and I love...          1\n",
       "4  I bought this for my grand daughter when she c...          1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    32315\n",
       "0      812\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33127, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 812 negative reviews, and 32315 positive reviews. This is very imbalanced, so to correct this we will downsammple the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsample majority:\n",
    "(Citation: https://elitedatascience.com/imbalanced-classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1500\n",
      "0     812\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2312, 2)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = reviews_df[reviews_df.Sentiment==1]\n",
    "df_minority = reviews_df[reviews_df.Sentiment==0]\n",
    " \n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,\n",
    "                                 n_samples=1500,\n",
    "                                 random_state=123)\n",
    " \n",
    "# Combine minority class with downsampled majority class\n",
    "df_downsampled = pd.concat([df_majority_downsampled, df_minority])\n",
    " \n",
    "# Display new class counts\n",
    "print(df_downsampled.Sentiment.value_counts())\n",
    "\n",
    "reviews_df=df_downsampled\n",
    "reviews_df.reset_index(inplace=True)\n",
    "reviews_df.drop(['index'], axis=1, inplace=True)\n",
    "reviews_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have 1500 positive reviews and 812 negative reviews, which is much more balanced than we had initially. Another option would have been to upsample the minority class, but because our minority class was so much smaller it would have resulted in many duplicate rows of our minority class and likely led to overfitting and lack of generalizability. We decided downsampling was a better approach, even though it resulted in quite a lot of lost data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seems to just be a novelty. which has worn off...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Christmas gift.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This Amazon Fire is perfect for what I need - ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This was perfect for entertaining my 20-month ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It is very easy to set up and use. It has grea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0  seems to just be a novelty. which has worn off...          1\n",
       "1                                    Christmas gift.          1\n",
       "2  This Amazon Fire is perfect for what I need - ...          1\n",
       "3  This was perfect for entertaining my 20-month ...          1\n",
       "4  It is very easy to set up and use. It has grea...          1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Review       0\n",
       "Sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle dataset so sentiments are not grouped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_df = reviews_df.sample(frac=1, random_state=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bestbuy came through before the holiday better...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alexa is a key component for easy smart home s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we're early in the stages of home smartness bu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Got this over the google home because I felt i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I paired this with the logitech elite universa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Sentiment\n",
       "0  Bestbuy came through before the holiday better...          0\n",
       "1  Alexa is a key component for easy smart home s...          1\n",
       "2  we're early in the stages of home smartness bu...          1\n",
       "3  Got this over the google home because I felt i...          1\n",
       "4  I paired this with the logitech elite universa...          1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bestbuy came through before the holiday better...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alexa is a key component for easy smart home s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we're early in the stages of home smartness bu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Got this over the google home because I felt i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I paired this with the logitech elite universa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I spent more time trying get it functioning th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Easy for a 5 year old to learn.. the apps on t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sure I can use my iPhone for internet browsing...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>You have to pay for every thing -any thing you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I bought it as a gift for my husband and he lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Very convenient and far from costly. It brings...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I absolutely love this Amazon Fire TV!! It's p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I visited a friend who had one and I was sold ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Tablet was working perfectly. Now the charging...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I love using Alexa. She will play music when a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>It is easy to operate and happy with Amazon Fi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>For the price, they are highly functional. Bou...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Bought for kids. Happy with purchase.May get o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Excellent sound quality and voice response fro...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I like my echo because of the things it can do...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Review  Sentiment\n",
       "0   Bestbuy came through before the holiday better...          0\n",
       "1   Alexa is a key component for easy smart home s...          1\n",
       "2   we're early in the stages of home smartness bu...          1\n",
       "3   Got this over the google home because I felt i...          1\n",
       "4   I paired this with the logitech elite universa...          1\n",
       "5   I spent more time trying get it functioning th...          0\n",
       "6   Easy for a 5 year old to learn.. the apps on t...          1\n",
       "7   Sure I can use my iPhone for internet browsing...          1\n",
       "8   You have to pay for every thing -any thing you...          0\n",
       "9   I bought it as a gift for my husband and he lo...          1\n",
       "10  Very convenient and far from costly. It brings...          1\n",
       "11  I absolutely love this Amazon Fire TV!! It's p...          1\n",
       "12  I visited a friend who had one and I was sold ...          1\n",
       "13  Tablet was working perfectly. Now the charging...          0\n",
       "14  I love using Alexa. She will play music when a...          1\n",
       "15  It is easy to operate and happy with Amazon Fi...          1\n",
       "16  For the price, they are highly functional. Bou...          1\n",
       "17  Bought for kids. Happy with purchase.May get o...          1\n",
       "18  Excellent sound quality and voice response fro...          1\n",
       "19  I like my echo because of the things it can do...          1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining our tokenizers: Clean the text to remove punctuation (except emoticons), put everything in lower case, and tokenize. These 2 tokenizers will be used in our grid search, and 1 will be chosen for our final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    return text.split()\n",
    "\n",
    "def tokenizerporter(text):\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    return [porter.stem(word) for word in text.split()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create vectorizer to use in gridsearch below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['analyzer', 'binary', 'decode_error', 'dtype', 'encoding', 'input', 'lowercase', 'max_df', 'max_features', 'min_df', 'ngram_range', 'norm', 'preprocessor', 'smooth_idf', 'stop_words', 'strip_accents', 'sublinear_tf', 'token_pattern', 'tokenizer', 'use_idf', 'vocabulary'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "tfidf.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Optimization and Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50/50 train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = reviews_df.loc[:812, 'Review'].values\n",
    "y_train = reviews_df.loc[:812, 'Sentiment'].values\n",
    "X_test = reviews_df.loc[812:, 'Review'].values\n",
    "y_test = reviews_df.loc[812:, 'Sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search to find optimal hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 1), (1, 2)],  # unigrams or bigrams\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizerporter],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__n_iter_no_change': [3,5,7,9],\n",
    "               'clf__alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3]},\n",
    "              {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "               'vect__stop_words': [stop, None],\n",
    "               'vect__tokenizer': [tokenizer, tokenizerporter],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__n_iter_no_change': [3,5,7,9],\n",
    "               'clf__alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3]},\n",
    "              ]\n",
    "\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', SGDClassifier(loss='log', random_state=1))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=2,\n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1024 candidates, totalling 5120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 6 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    4.7s\n",
      "[Parallel(n_jobs=-1)]: Done 150 tasks      | elapsed:   19.9s\n",
      "[Parallel(n_jobs=-1)]: Done 353 tasks      | elapsed:   42.6s\n",
      "[Parallel(n_jobs=-1)]: Done 636 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1001 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1446 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 1973 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 2580 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3269 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done 4038 tasks      | elapsed: 11.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4889 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=-1)]: Done 5120 out of 5120 | elapsed: 13.6min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=False,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_acc...\n",
       "                                                \"you'll\", \"you'd\", 'your',\n",
       "                                                'yours', 'yourself',\n",
       "                                                'yourselves', 'he', 'him',\n",
       "                                                'his', 'himself', 'she',\n",
       "                                                \"she's\", 'her', 'hers',\n",
       "                                                'herself', 'it', \"it's\", 'its',\n",
       "                                                'itself', ...],\n",
       "                                               None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x7fa9803ad680>,\n",
       "                                              <function tokenizerporter at 0x7fa9803ad830>],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__alpha': 0.0001, 'clf__n_iter_no_change': 5, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x7fa9803ad680>} \n",
      "CV Accuracy: 0.872\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('CV Accuracy: %.3f' % gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.851\n"
     ]
    }
   ],
   "source": [
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Best parameters are: \n",
    "<br>'clf__alpha': 0.0001\n",
    "<br>'clf__penalty': 'l2'\n",
    "<br>'clf__n_iter_no_change': 5\n",
    "<br>'vect__ngram_range': (1, 1)\n",
    "<br>'vect__stop_words': None\n",
    "<br>'vect__tokenizer': tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We included ngram_range as a hyperparameter because the vectorizer will look at pairs of words instead of single words; we thought this might add value if the ways in which words appear next to one another has some meaning for sentiment. But our grid search showed (1,1) was better than (1,2) so we are just looking at single words. The book example used just (1,1) as the value, so adding (1,2) as an option for ours did not offer any improvement since (1,1) was still the best.\n",
    "    <br>We altered the 'C' parameter the book used to 'alpha' since we are fitting with a different algorithm. For our alpha value, we included a greater range of options than the book did with their 'C' value. The alpha was chosen as 0.0001, which is the default.\n",
    "    <br>We also added n_iter_no_change so that there will be early stopping if there is no improvement after a certain number of iterations (default is 5, so that's what the book example's grid was using). After the grid search, the best n_iter_no_change was found to be 5, which was the same as the default that the book used. So adding this to the grid did not change the model performance. \n",
    "    <br>In conclusion, we would have gotten the same optimaal hyperparameters by using the same grid as the book (except changing C to alpha)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define HashingVectorizer with parameters from grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         ngram_range=(1,1),\n",
    "                         stop_words=None,\n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit streaming model with chosen parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.version import LooseVersion as Version\n",
    "from sklearn import __version__ as sklearn_version\n",
    "\n",
    "def stream_docs(df):\n",
    "    for index, row in df.iterrows():\n",
    "        text, label = row['Review'], row['Sentiment']\n",
    "        yield text, label\n",
    "        \n",
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y\n",
    "\n",
    "clf = SGDClassifier(loss='log', random_state=1, penalty='l2', alpha=0.0001)\n",
    "\n",
    "doc_stream = stream_docs(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:01\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=20)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.850\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=20)\n",
    "X_test = vect.transform(X_test)\n",
    "print('Accuracy: %.3f' % clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model and stop words to pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "dest = os.path.join('website', 'pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "\n",
    "pickle.dump(stop, open(os.path.join(dest, 'stopwords.pkl'), 'wb'), protocol=4)   \n",
    "pickle.dump(clf, open(os.path.join(dest, 'amazon_classifier.pkl'), 'wb'), protocol=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write tokenizer and hashing vectorizer to .py file called amazonreview_vectorizer.py (we included both tokenizers and the stop words in case future hyperparameter tuning would require it, though currently we are only using the simple tokenizer and no stop words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting website/amazonreview_vectorizer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile website/amazonreview_vectorizer.py \n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "\n",
    "porter=PorterStemmer()\n",
    "\n",
    "cur_dir = os.path.dirname(__file__)\n",
    "stop = pickle.load(open(\n",
    "                os.path.join(cur_dir, \n",
    "                'pkl_objects', \n",
    "                'stopwords.pkl'), 'rb'))\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    return text.split()\n",
    "\n",
    "def tokenizerporter(text):\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "vect = HashingVectorizer(decode_error='ignore',\n",
    "                         n_features=2**21,\n",
    "                         preprocessor=None,\n",
    "                         ngram_range=(1,1),\n",
    "                         stop_words=None,\n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('website')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "from amazonreview_vectorizer import vect\n",
    "\n",
    "model = pickle.load(open(os.path.join('pkl_objects', 'amazon_classifier.pkl'), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=1000,\n",
       "              n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n",
       "              random_state=1, shuffle=True, tol=0.001, validation_fraction=0.1,\n",
       "              verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are a few test samples of new reviews and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: positive\n",
      "Probability: 96.89%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "label = {0:'negative', 1:'positive'}\n",
    "\n",
    "example = [\"I love this product. It's amazing.\"]\n",
    "X = vect.transform(example)\n",
    "print('Prediction: %s\\nProbability: %.2f%%' %\\\n",
    "      (label[model.predict(X)[0]], \n",
    "       np.max(model.predict_proba(X))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: negative\n",
      "Probability: 91.95%\n"
     ]
    }
   ],
   "source": [
    "label = {0:'negative', 1:'positive'}\n",
    "\n",
    "example = [\"bad bad bad terrible awful bad don't hate loser awful regret\"]\n",
    "X = vect.transform(example)\n",
    "print('Prediction: %s\\nProbability: %.2f%%' %\\\n",
    "      (label[model.predict(X)[0]], \n",
    "       np.max(model.predict_proba(X))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: positive\n",
      "Probability: 99.88%\n"
     ]
    }
   ],
   "source": [
    "label = {0:'negative', 1:'positive'}\n",
    "\n",
    "example = [\"amazing i love it so much and great happy again love yes yay like smile\"]\n",
    "X = vect.transform(example)\n",
    "print('Prediction: %s\\nProbability: %.2f%%' %\\\n",
    "      (label[model.predict(X)[0]], \n",
    "       np.max(model.predict_proba(X))*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: negative\n",
      "Probability: 96.91%\n"
     ]
    }
   ],
   "source": [
    "label = {0:'negative', 1:'positive'}\n",
    "\n",
    "example = [\"too slow for me you tube don t work sometimes\"]\n",
    "X = vect.transform(example)\n",
    "print('Prediction: %s\\nProbability: %.2f%%' %\\\n",
    "      (label[model.predict(X)[0]], \n",
    "       np.max(model.predict_proba(X))*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Website Creation and Publishing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up SQLite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/projects/project02/website'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "conn = sqlite3.connect('reviews.sqlite')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('DROP TABLE IF EXISTS review_db')\n",
    "c.execute('CREATE TABLE review_db (review TEXT, sentiment INTEGER, date TEXT)')\n",
    "\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Link to Python Anywhere website:\n",
    "http://anndirussell.pythonanywhere.com/?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
